{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf57c55",
   "metadata": {},
   "source": [
    "# Cognitive NPL Resolution Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ff3e6",
   "metadata": {},
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c792abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import category_encoders as ce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb395e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/tri/Documents/code/P02/notebook/../mlruns/574172305759250793', creation_time=1771514225988, experiment_id='574172305759250793', last_update_time=1771514225988, lifecycle_stage='active', name='CreditScoring', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mlflow\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "\n",
    "mlflow.set_experiment(\"CreditScoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba683a",
   "metadata": {},
   "source": [
    "### Config & Settings Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c585b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data_paths\": {\n",
    "        \"raw\": \"../data/01-raw/\",\n",
    "        \"processed\": \"../data/02-processed/\",\n",
    "        \"features\": \"../data/03-features/\",\n",
    "    },\n",
    "    \"model_paths\": {\n",
    "        \"artifacts\": \"../models/\"\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"input_data\": \"data_clean.parquet\",\n",
    "        \"train_out\": \"feature_train.parquet\",\n",
    "        \"test_out\": \"feature_test.parquet\"\n",
    "    },\n",
    "    \"target_col\": \"is_default\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \n",
    "    \"b2c_naics_codes\": ['44', '45', '61', '62', '71', '72', '81']\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"data_paths\"][\"features\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"model_paths\"][\"artifacts\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abe35d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314c2a3",
   "metadata": {},
   "source": [
    "### Feature Construction/Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0697648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(df):\n",
    "    \n",
    "    if 'is_default' not in df.columns:\n",
    "        df = df[df['loanstatus'].isin(['PIF', 'CHGOFF'])].copy()\n",
    "        df['is_default'] = df['loanstatus'].apply(lambda x: 1 if x == 'CHGOFF' else 0)\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    gdp_map = df[['Industry_BEA', 'Quarter_Date', 'GDP_Value']].drop_duplicates().sort_values(['Industry_BEA', 'Quarter_Date'])\n",
    "    gdp_map['gdp_pct_change'] = gdp_map.groupby('Industry_BEA')['GDP_Value'].pct_change()\n",
    "    df = df.merge(gdp_map[['Industry_BEA', 'Quarter_Date', 'gdp_pct_change']], on=['Industry_BEA', 'Quarter_Date'], how='left')\n",
    "    df['is_sector_in_recession'] = (df['gdp_pct_change'] < 0).astype(int) #H1\n",
    "    \n",
    "    df['is_b2c'] = df['NAICS_2'].astype(str).isin(CONFIG['b2c_naics_codes']).astype(int)\n",
    "    df['unrate_b2c_impact'] = df['UNRATE'] * df['is_b2c'] \n",
    "    \n",
    "    df['is_high_rate_era'] = (df['DPRIME'] <= 4.0).astype(int) #H3\n",
    "    \n",
    "    prime_map = df[['Quarter_Date', 'DPRIME']].drop_duplicates().sort_values('Quarter_Date')\n",
    "    prime_map['DPRIME_Lag_7'] = prime_map['DPRIME'].shift(7)\n",
    "    df = df.merge(prime_map[['Quarter_Date', 'DPRIME_Lag_7']], on='Quarter_Date', how='left')\n",
    "  \n",
    "    df['is_microloan'] = (df['grossapproval'] < 50000).astype(int) #H4\n",
    "    \n",
    "    high_risk_states = ['HI', 'NY', 'DE']\n",
    "    df['is_high_risk_state'] = df['borrstate'].isin(high_risk_states).astype(int) #H5\n",
    "\n",
    "    df['is_short_term'] = (df['terminmonths'] < 36).astype(int) #H6\n",
    "    df['log_grossapproval'] = np.log1p(df['grossapproval'])\n",
    "    df['loan_to_jobs_ratio'] = df['grossapproval'] / (df['jobssupported'] + 1)\n",
    "    \n",
    "    if 'franchisename' in df.columns:\n",
    "        df['is_franchise'] = df['franchisename'].notna().astype(int)\n",
    "    \n",
    "    # Kembalikan urutan berdasarkan waktu persetujuan pinjaman\n",
    "    df = df.sort_values('approvaldate').reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc87866",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc70ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_col):\n",
    "    \n",
    "    # Drop kolom yang tidak relevan / menyebabkan bocor (Data Leakage)\n",
    "    drop_cols = ['l2locid', 'borrname', 'borrstreet', 'borrcity', 'borrzip', \n",
    "                 'bankname', 'bankstreet', 'bankcity', 'bankzip', \n",
    "                 'firstdisbursementdate', 'paidinfulldate', 'chargeoffdate', \n",
    "                 'observation_date', 'observation_date_unrate', 'Quarter_Date', \n",
    "                 'loanstatus', 'naicsdescription', 'franchisename', 'asofdate'\n",
    "                 'subpgmdesc']\n",
    "    \n",
    "    df_model = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "    \n",
    "    df_model = df_model.dropna(subset=[target_col, 'gdp_pct_change', 'DPRIME_Lag_7'])\n",
    "    \n",
    "    split_idx = int(len(df_model) * (1 - CONFIG['test_size']))\n",
    "    \n",
    "    train_df = df_model.iloc[:split_idx]\n",
    "    test_df = df_model.iloc[split_idx:]\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_col, 'approvaldate'])\n",
    "    y_train = train_df[target_col]\n",
    "    \n",
    "    X_test = test_df.drop(columns=[target_col, 'approvaldate'])\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0264561",
   "metadata": {},
   "source": [
    "### Scaling & Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d050ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X_train, X_test, y_train):\n",
    "    \n",
    "    cat_cols_high_card = ['borrstate', 'bankstate', 'projectstate', 'Industry_BEA', 'NAICS_2']\n",
    "    cat_cols_low_card = ['program', 'processingmethod', 'collateralind']\n",
    "    binary_cols = ['is_b2c', 'is_cheap_money_era', 'is_microloan', 'is_high_risk_state', 'is_short_term', 'is_sector_in_recession', 'is_franchise']\n",
    "    \n",
    "    cat_cols_high_card = [c for c in cat_cols_high_card if c in X_train.columns]\n",
    "    cat_cols_low_card = [c for c in cat_cols_low_card if c in X_train.columns]\n",
    "    binary_cols = [c for c in binary_cols if c in X_train.columns]\n",
    "    num_cols = [c for c in X_train.columns if c not in cat_cols_high_card + cat_cols_low_card + binary_cols and is_numeric_dtype(X_train[c])]\n",
    "\n",
    "    # Target Encoding (High Cardinality)\n",
    "    target_enc = ce.TargetEncoder(cols=cat_cols_high_card)\n",
    "    X_train_te = target_enc.fit_transform(X_train[cat_cols_high_card], y_train)\n",
    "    X_test_te = target_enc.transform(X_test[cat_cols_high_card])\n",
    "    joblib.dump(target_enc, f\"{CONFIG['model_paths']['artifacts']}target_encoder.pkl\")\n",
    "\n",
    "    # One Hot Encoding (Low Cardinality)\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first') \n",
    "    X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[cat_cols_low_card]), columns=ohe.get_feature_names_out())\n",
    "    X_test_ohe = pd.DataFrame(ohe.transform(X_test[cat_cols_low_card]), columns=ohe.get_feature_names_out())\n",
    "    X_train_ohe.index = X_train.index\n",
    "    X_test_ohe.index = X_test.index\n",
    "    joblib.dump(ohe, f\"{CONFIG['model_paths']['artifacts']}ohe_encoder.pkl\")\n",
    "\n",
    "    # Scaling \n",
    "    scaler = RobustScaler()\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test[num_cols]), columns=num_cols)\n",
    "    X_train_num.index = X_train.index\n",
    "    X_test_num.index = X_test.index\n",
    "    joblib.dump(scaler, f\"{CONFIG['model_paths']['artifacts']}robust_scaler.pkl\")\n",
    "\n",
    "    X_train_final = pd.concat([X_train_num, X_train_te, X_train_ohe, X_train[binary_cols]], axis=1)\n",
    "    X_test_final = pd.concat([X_test_num, X_test_te, X_test_ohe, X_test[binary_cols]], axis=1)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7157534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Engineer Features...\n",
      "[INFO] Splitting OOT (Out-of-Time)...\n",
      "[INFO] Preprocessing (Encoding & Scaling)...\n",
      "[INFO] Saving artifacts to /03-features/...\n",
      "[INFO] Logging Mlflow Artifacts..\n",
      "[SUCCESS] Feature Engineering & Preprocessing Completed! MLflow RUN ID: 554458798cf34f41943c57f1c2f6c8ff\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='preprocessing_v1') as run:\n",
    "\n",
    "    print(\"[INFO] Loading data...\")\n",
    "    df_raw = pd.read_parquet(f\"{CONFIG['data_paths']['processed']}{CONFIG['files']['input_data']}\")\n",
    "    df_raw = create_target_variable(df_raw)\n",
    "    \n",
    "    initial_cols = set(df_raw.columns)\n",
    "\n",
    "    print(\"[INFO] Engineer Features...\")\n",
    "    df_fe = engineer_features(df_raw)\n",
    "    \n",
    "    new_features = list(set(df_fe.columns) - initial_cols)\n",
    "    mlflow.log_param(\"Feature Derivative\", \",\".join(new_features))\n",
    "    mlflow.log_metric(\"Total Feature Derivative\", len(new_features))\n",
    "    mlflow.log_param(\"NAICS_Code\", CONFIG['b2c_naics_codes'])\n",
    "\n",
    "    print(\"[INFO] Splitting OOT (Out-of-Time)...\")\n",
    "    X_train, X_test, y_train, y_test = split_data(df_fe, CONFIG['target_col'])\n",
    "    \n",
    "    mlflow.log_param(\"test_size\", CONFIG['test_size'])\n",
    "    mlflow.log_param(\"split_strategy\",\"Out-of-Time/Chronological\")\n",
    "    mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "    mlflow.log_metric(\"test_samples\", len(X_test))\n",
    "    \n",
    "    mlflow.log_metric(\"train_default_rate\", y_train.mean())\n",
    "    mlflow.log_metric(\"test_default_rate\", y_test.mean())\n",
    "    \n",
    "    print(\"[INFO] Preprocessing (Encoding & Scaling)...\")\n",
    "    X_train_final, X_test_final = preprocess_features(X_train, X_test, y_train)\n",
    "    \n",
    "    mlflow.log_param(\"Scaler\", \"RobustScaler\")\n",
    "    mlflow.log_param(\"encoder_high_cardinality\", \"TargetEncoder\")\n",
    "    mlflow.log_param(\"encoder_low_cardinality\",\"OneHotEncoder\")\n",
    "    mlflow.log_metric(\"final_feature_count\", X_train_final.shape[1])\n",
    "\n",
    "    train_final = X_train_final.copy()\n",
    "    train_final['is_default'] = y_train.values\n",
    "\n",
    "    test_final = X_test_final.copy()\n",
    "    test_final['is_default'] = y_test.values\n",
    "\n",
    "    print(\"[INFO] Saving artifacts to /03-features/...\")\n",
    "    \n",
    "    train_path = f\"{CONFIG['data_paths']['features']}{CONFIG['files']['train_out']}\"\n",
    "    test_path = f\"{CONFIG['data_paths']['features']}{CONFIG['files']['test_out']}\"\n",
    "    \n",
    "    train_final.to_parquet(train_path, index=False)\n",
    "    test_final.to_parquet(test_path, index=False)\n",
    "    \n",
    "    print(\"[INFO] Logging Mlflow Artifacts..\")\n",
    "    mlflow.log_artifact(train_path, artifact_path=\"datasets\")\n",
    "    mlflow.log_artifact(test_path, artifact_path=\"datasets\")\n",
    "    \n",
    "    mlflow.log_artifacts(CONFIG['model_paths']['artifacts'], artifact_path='preprocessors')\n",
    "\n",
    "    print(f\"[SUCCESS] Feature Engineering & Preprocessing Completed! MLflow RUN ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e0e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemastik18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
